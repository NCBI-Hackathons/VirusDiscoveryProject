\section{Materials and Methods}

  \subsection{Participant Recruitment}
  After initial conception of this project by BB, RJ (JRB?) and RE, RE offered
  to provide a venue for an international hackathon.  Participants were
  recruited through the outreach efforts of BB, RJ, and RE.  VZ identified
  datasets, which were then parsed by RE using PARTIE \cite{Torres2017} to look at any potential
  amplicon or 16S character.  The resulting set of SRRs can be found in
  Supplemental File 1.


  \subsection{Assembling contigs from metagenomic datasets (pre-Hackathon)}
  Contigs containing putative virus sequences were assembled from metagenomic
  SRA datasets by removing human reads and assembling putative virus sequences
  into contigs using SKESA \cite{Souvorov2018}. All reads from an SRR archive
  were aligned against the human genome reference sequence (GRCh38.p12) using
  HISAT \cite{Kim2015} (--no-spliced-alignment --no-discordant
  guidedassembler\_graph options: --extend\_ends  --word 11 --seed\_prec 6
  --min\_hit\_len 1000 --fraction 0.1 --no\_filter\_by\_reads
  --no\_filter\_by\_pairs). Reads mapping fully or partially  to the human
  genome were  classified as 'human'. Putative virus sequences in the remaining
  reads were identified using a K-mer taxonomy approach, using a minimum
  threshold of 1,000 hits at the viral species level. The remaining NCBI
  taxonomy identifiers were used to extract sequences from Refseq. Given that
  some viruses are overrepresented in RefSeq,  only a few per species were
  selected at random while for viruses with segmented genomes, e.g. Influenza,
  all sequences were selected and deduplicated by k-mer distances in a later
  step using MASH \cite{Ondov2019}. Putative virus reads were assembled using
  the guided\_assembler from the SKESA (-hash\_count) and these contigs
  obtained identifiers based on the guide accessions with a sequential index as
  a suffix, for example NC\_006883.2\_1 is based on Prochlorococcus phage
  genome NC\_006883.2.In cases where guide selection failed to detect good
  reference sets a default viral reference set was used based on ViralZone
  database \cite{Hulo2011}.

  Reads not classified as virus or human were de-novo assembled with SKESA. For
  the assembled runs the de novo contigs served as a reference set to align the
  reads with HISAT2 (as above). The reads that didn't align onto either human,
  viral or de-novo contigs were classified as unknown. As a result of the
  workflow each run was re-aligned onto human, viral and de-novo contigs and
  contains the same set of reads as the original run. The alignments were
  converted into SRA format without quality scores and stored in google cloud
  storage for later analysis. Given that most SRA metagenomic reads are
  bacterial or of unknown origin this step was the most computationally
  intensive with significant memory and runtime requirements. Due to the
  limited budget a timeout was introduced on de-novo assembly step and some
  runs failed to complete.

  \subsection{GCP (Google Cloud Computing)}
  Cloud BLAST
  for knowns group
  All commands for pulling contig .fasta files from Google buckets, BLASTn, and classification of output into the four classification groups are available on GitHub at: https://github.com/NCBI-Hackathons/VirusDiscoveryProject/tree/master/KnownViruses
  for domains group

  \subsection{Clustering}
  Contigs and Refseq virus nucleotide sequences were stored in a single flat
  file. Coding-complete, genomic viral sequences Virus sequences were extracted
  from the NCBI Entrez Nucleotide database
  (\url{https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?VirusLineage_ss=Viruses,%20taxid:10239&SeqType_s=Nucleotide})
  to create a specific database using the makeblastdb command-line tool
  \cite{Camacho2009}. And all sequences were  compared against all sequences
  using MEGABLAST \cite{Camacho2009} with an E-value cut-off of $1e^{-10}$ and
  a maximum of one contiguously-aligned region (High-scoring Segment pair, HSP)
  per query-subject pair.


  \subsection{Markov Clustering}
  Markov Clustering (MCL, \cite{Enright2002}) was applied to blast results as
  outlined in the associated documentation (\url{https://micans.org/mcl/}).
  Briefly, tabular blast output was modified to include only qacc, sacc, and
  evalue columns, and passed to mcxload to generate network and dictionary
  files. Thus the set of query and subject pairs is treated as the edge set for
  a graph, the associated evalues are treated as edge weights. The
  stream-mirror argument was used to ensure the network is undirected, and
  stream-neg-log10  and stream-tf 'ceil(200)' arguments were used to log
  transform the evalues, setting a maximum value of 200 for edge weights.
  Finally, the mcl algorithm was run on the loaded network with an inflation
  value of 10, and 32 threads. All MCL work was performed on a GCP machine with
  96 cores and 240 Gb RAM.

  \subsection{VIGA}
  Modifications were made to the standard VIGA \cite{Gonzalez-Tortuero2018}
  protocol to enhance the overall speed of the program, removing the rRNA
  detection step by INFERNAL \cite{Nawrocki2013}. This pipeline handled this
  information, enhancing the identification of viral specific hidden-Markov
  models (HMM) annotations by the utilization of the complete pVOG database
  \cite{Grazziotin2017} (9,518 HMMs) and the addition of RVDB
  \cite{Goodacre2018} using HMMER suite \cite{hmmer}. Modified scripts and
  instructions to reproduce all steps are available on GitHub at
  \url{https://github.com/NCBI-Hackathons/VirusDiscoveryProject/tree/master/VirusGenes}.
  All viral annotation was performed on a GCP machine with 160 cores and XX Gb
  RAM
